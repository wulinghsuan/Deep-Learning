{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSTI_DL_Lab4_March2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXfVRw5p4YOo",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network on GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT49HHnBMPTh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "From Kaggle: \n",
        "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
        "\n",
        "[Read more.](https://www.kaggle.com/c/digit-recognizer)\n",
        "\n",
        "\n",
        "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-Iyt4NXJoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq6RJyDIOB-T",
        "colab_type": "text"
      },
      "source": [
        "## STEP 1: LOADING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL21SXchOBwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_zuJQxSOKQt",
        "colab_type": "text"
      },
      "source": [
        "## STEP 2: MAKING DATASET ITERABLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bGDbP1pL4XE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "n_iters = 1200 #3000 \n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FEft3CQOQIR",
        "colab_type": "text"
      },
      "source": [
        "## STEP 3: CREATE MODEL CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbEJ-1aAL9d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        # Max pool 1\n",
        "        out = self.maxpool1(out)\n",
        "\n",
        "        # Convolution 2 \n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "\n",
        "        # Max pool 2 \n",
        "        out = self.maxpool2(out)\n",
        "\n",
        "        # Resize\n",
        "        # Original size: (100, 32, 7, 7)\n",
        "        # out.size(0): 100\n",
        "        # New out size: (100, 32*7*7)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3owYgg2OWYw",
        "colab_type": "text"
      },
      "source": [
        "## STEP 4: INSTANTIATE MODEL CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dqM6gd4MA2I",
        "colab_type": "code",
        "outputId": "4e8dff6a-9c9f-46cf-a324-bdf5e49590cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model = CNNModel()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "print(device)\n",
        "model.to(device)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNModel(\n",
              "  (cnn1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfSU4Nn7GfXS",
        "colab_type": "code",
        "outputId": "12e7898c-a51b-4f9e-d092-28ec96fc3983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNNModel(\n",
            "  (cnn1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ArDN205ObQG",
        "colab_type": "text"
      },
      "source": [
        "## STEP 5: INSTANTIATE LOSS CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtqarS-WMC5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqikNfyhOe7o",
        "colab_type": "text"
      },
      "source": [
        "## STEP 6: INSTANTIATE OPTIMIZER CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1roaQQwYMFFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "#all parameters are on GPU\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsAaSLaaGrMZ",
        "colab_type": "code",
        "outputId": "ace4fbce-7728-4409-b373-d61ea1003107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(model.parameters)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.parameters of CNNModel(\n",
            "  (cnn1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (cnn2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0A9mJ3d6-f8",
        "colab_type": "text"
      },
      "source": [
        "Function to compute the accuracy on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rscNQPz_AfF",
        "colab_type": "text"
      },
      "source": [
        "### Question: modify the following code to exploit both the GPU and the CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv17qSuGO5x6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(test_loader, model, device):\n",
        "  # Calculate Accuracy         \n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Iterate through test dataset\n",
        "  for images, labels in test_loader:\n",
        "    images = images.requires_grad_().to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass only to get logits/output\n",
        "    outputs = model(images)\n",
        "\n",
        "    # Get predictions from the maximum value\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Total number of labels\n",
        "    total += labels.size(0)\n",
        "\n",
        "    # Total correct predictions\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(accuracy)\n",
        "    \n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJWjmbnOhff",
        "colab_type": "text"
      },
      "source": [
        "## STEP 7: TRAIN THE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZh9Bryn-15o",
        "colab_type": "text"
      },
      "source": [
        "### Question: modify the following code to exploit the GPU instead of the CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ADmEX6UMINe",
        "colab_type": "code",
        "outputId": "8dba8574-8aaf-4fb4-82b8-813aec391b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "%%time\n",
        "# Time execution of a Python statement or expression.\n",
        "# wall time is the actual time taken from the start of a computer program to the end\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader): # on CPU \n",
        "\n",
        "        # Transfer to GPU\n",
        "        images = images.requires_grad_().to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy on the test set        \n",
        "            accuracy = test_model(test_loader, model,device)\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy on test set: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(89, device='cuda:0')\n",
            "Iteration: 500. Loss: 0.31570589542388916. Accuracy on test set: 89\n",
            "tensor(92, device='cuda:0')\n",
            "Iteration: 1000. Loss: 0.25182855129241943. Accuracy on test set: 92\n",
            "CPU times: user 19.7 s, sys: 1.92 s, total: 21.6 s\n",
            "Wall time: 21.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozPdNeJH-Tfq",
        "colab_type": "text"
      },
      "source": [
        "### Question: compare the wall time on GPU to the wall time on CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_8lEHW3nkrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "da3797ec-c33e-47eb-e36d-626cba7e78b1"
      },
      "source": [
        "%%time\n",
        "# Time execution of a Python statement or expression.\n",
        "# wall time is the actual time taken from the start of a computer program to the end\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader): # on CPU \n",
        "\n",
        "        images = images.requires_grad_()\n",
        "        labels = labels\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy on the test set        \n",
        "            accuracy = test_model(test_loader, model,device)\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy on test set: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(87)\n",
            "Iteration: 500. Loss: 0.39787793159484863. Accuracy on test set: 87\n",
            "tensor(92)\n",
            "Iteration: 1000. Loss: 0.2556685209274292. Accuracy on test set: 92\n",
            "CPU times: user 1min 7s, sys: 1.18 s, total: 1min 9s\n",
            "Wall time: 1min 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gekTtS1p_v2k",
        "colab_type": "text"
      },
      "source": [
        "### Question: increase the number of epoch until 5 to see if we can expect a better average accuracy"
      ]
    }
  ]
}